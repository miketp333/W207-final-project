{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHSAT Test Results Merge Notebook\n",
    "[Return to project overview](final_project_overview.ipynb)\n",
    "\n",
    "### Andrew Larimer, Deepak Nagaraj, Daniel Olmstead, Michael Winton (W207-4-Summer 2018 Final Project)\n",
    "\n",
    "In this notebook, we will merge the data cleaned by the other \"prep_\" notebooks to create a single merged csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing dataframes, indexed by our primary key\n",
    "While school names may change or be input inconsistently, each school has a unique identifying DBN, sometimes referred to as a Location Code, to identify it. By importing each cleaned dataset with the DBN as the index, we are able to easily join them into a merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all datasets from CSV; when loading set index to the DBN column (to enforce uniqueness)\n",
    "shsat_df = pd.read_csv('data_cleaned/cleaned_shsat_outcomes.csv', index_col=\"dbn\")\n",
    "print('SHSAT dataset:',shsat_df.shape) # confirm that it's (589, 5)\n",
    "\n",
    "class_sizes_df = pd.read_csv('data_cleaned/cleaned_class_sizes.csv', index_col=\"dbn\")\n",
    "print('Class size dataset:', class_sizes_df.shape) # confirm that it's (494,13)\n",
    "\n",
    "explorer_df = pd.read_csv('data_cleaned/cleaned_explorer.csv', index_col=\"dbn\")\n",
    "print('Explorer dataset:', explorer_df.shape) # confirm that it's (596, 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for duplicate entries.\n",
    "We do a quick check to make sure there are no duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shsat_dups = shsat_df.index.duplicated()\n",
    "class_sizes_dups = class_sizes_df.index.duplicated()\n",
    "explorer_dups = explorer_df.index.duplicated()\n",
    "                            \n",
    "print(\"True or False: there are duplicated indices within any dataframes?\")\n",
    "print(\"{0}.\".format(bool(sum(shsat_dups) + sum(class_sizes_dups) + sum(explorer_dups))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner joins for more complete data\n",
    "We'll use inner joins to select the intersection of our datasets, thus only selecting for schools for which we have data from each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = shsat_df.join(explorer_df, how=\"inner\")\n",
    "merged_df = merged_df.join(class_sizes_df, how=\"inner\")\n",
    "print(\"Merged Dataframe shape:\",merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still leaves us with a merged dataframe of 464 rows and 66 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating density\n",
    "Let's take a look at how sparse our data is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total empty cells:\",merged_df.isnull().sum().sum())\n",
    "print(\"Percent null: {0:.3f}%\".format(100*merged_df.isnull().sum().sum()/(merged_df.shape[0]*merged_df.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our worst offending rows and columns to see if anything stands out enough to be removed:\n",
    "\n",
    "### Columns with Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isnull().sum()[merged_df.isnull().sum() > 0]\\\n",
    "    .sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rows with Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isnull().sum(axis=1)[merged_df.isnull().sum(axis=1) > 0]\\\n",
    "    .sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment we don't see any of these as being offending enough to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a dated file\n",
    "\n",
    "To allow updates to the merged dataframe without disrupting work on models downstream until they are ready, we save a dated merged filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the date to create the filename.\n",
    "d = datetime.date\n",
    "filename = \"combined_data_{0}.csv\".format( d.today().isoformat() )\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check final shape (464,66)\n",
    "merged_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"data_merged/{0}\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
