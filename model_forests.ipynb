{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests Notebook\n",
    "[Return to project overview](final_project_overview.ipynb),\n",
    "\n",
    "### Andrew Larimer, Deepak Nagaraj, Daniel Olmstead, Michael Winton (W207-4-Summer 2018 Final Project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and setting options\n",
    "\n",
    "First we import necessary libraries, including our util functions, and set Pandas and Matplotlib options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from util import our_train_test_split, read_data, get_dummies, print_cv_results\n",
    "import pickle\n",
    "\n",
    "# set default options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading our data using util cleanup and imputing\n",
    "\n",
    "Our util module has shared utility functions for cleaning up our data and imputing means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the cleaned, merged, and mean-imputed data from our utility function\n",
    "train_data, test_data, train_labels, test_labels = read_data(do_imputation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll drop some columns that were used to calculate our dependendt variable, as well as our index column, school name strings, and `school_income_estimate` which had too many missing values to fill via imputing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop a few features for the following reasons:\n",
    "#    Used in generating dependent variable: 'num_shsat_test_takers',\n",
    "#        'offers_per_student', 'pct_test_takers'\n",
    "#    Strings or other non-features: 'dbn', 'school_name'\n",
    "#    Too many empty values: 'school_income_estimate'\n",
    "#    Data preserved in other features: 'zip', 'rigorous_instruction_rating',\n",
    "#       'collaborative_teachers_rating', 'supportive_environment_rating',\n",
    "#       'effective_school_leadership_rating',\n",
    "#       'strong_family_community_ties_rating', 'trust_rating'\n",
    "#    Found not to help model: 'district' (or one-hot encoding)\n",
    "\n",
    "FEATURES_TO_DROP = ['dbn', 'school_name', 'zip', 'num_shsat_test_takers',\n",
    "                    'offers_per_student', 'pct_test_takers', 'school_income_estimate',\n",
    "                    'rigorous_instruction_rating','collaborative_teachers_rating',\n",
    "                    'supportive_environment_rating',\n",
    "                    'effective_school_leadership_rating',\n",
    "                    'strong_family_community_ties_rating', 'trust_rating',\n",
    "                    'district']\n",
    "\n",
    "# We'll go ahead and drop total_columns_to_drop columns.\n",
    "train_prepped = train_data.drop(FEATURES_TO_DROP,axis=1)\n",
    "test_prepped = test_data.drop(FEATURES_TO_DROP,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We confirm our resulting data has no more NAs\n",
    "print(\"Confirm total of remaining NAs is: \",np.sum(np.sum(train_dropped.isna())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing a Random Forest Model on Cross-Validation\n",
    "\n",
    "We now move into training our random forest model. To optimize our hyperparameter of how many trees to include in our forest, we use GridSearchCV and take advantage of its cross validation capability to use cross validation against our training set instead of further reducing our data into smaller train and dev sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check for previously saved results and a serialized model saved\n",
    "# to disk before re-running GridSearchCV. To force it to run again,\n",
    "# we can comment out the try: & except: or just delete the last saved\n",
    "# results.\n",
    "\n",
    "try:\n",
    "\n",
    "    cv_results = pd.read_csv('cache_forest/forest_gridsearch_results.csv')\n",
    "    with open('cache_forest/pickled_forest','rb') as f:\n",
    "        forest_cv = pickle.load(f)\n",
    "\n",
    "except:\n",
    "\n",
    "    # If no saved results are found, we define our base Random Forest\n",
    "    # Classifier with fixed parameters we don't anticipate adjusting.\n",
    "    # We want to run as many jobs as we have cores at once (which is\n",
    "    # what the -1 input to n_jobs does, and we define our random state\n",
    "    # for reproducibility.)\n",
    "    forest = RandomForestClassifier(n_jobs=-1, class_weight='balanced',\n",
    "                                    random_state=207)\n",
    "\n",
    "    # We define a range of paramters we'd like to try for our forest.\n",
    "    params_to_try = {'n_estimators':[10,30,100,300],\n",
    "                     'max_depth':[None,2,5,7],\n",
    "                     'min_samples_leaf':[1,2,4],\n",
    "                     'min_samples_split':[2,3,5],\n",
    "                     'max_features':[.2,.5,.8]}\n",
    "\n",
    "    # We define into how many groups we'd like to split our test data\n",
    "    # for use in cross-validation to evaluate hyperparameters.\n",
    "    KFOLDS = 5\n",
    "\n",
    "    # Now we run GridSearchCV on our forest estimator, trying our varying\n",
    "    # numbers of trees and utilizing our cross validation to determine the\n",
    "    # best number of trees across the best number of train/dev cross\n",
    "    # validation splits, using a weighted F1 score as our metric of success.\n",
    "    forest_cv = GridSearchCV(forest, params_to_try, scoring=['f1',\n",
    "                            'accuracy'], refit='f1', cv=KFOLDS,\n",
    "                             return_train_score=False)\n",
    "\n",
    "    # We'll time it and report how long it took to run:\n",
    "    start_time = time.time()\n",
    "    forest_cv.fit(train_prepped, train_labels)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    took = int(end_time - start_time)\n",
    "    print(\"Grid search took {0:d} minutes, {1:d} seconds.\".format(\n",
    "              took // 60, took % 60))\n",
    "\n",
    "    # And pickle our trained model, and save our scores to csv.\n",
    "    with open('cache_forest/pickled_forest','wb') as f:\n",
    "        pickle.dump(forest_cv, f)\n",
    "\n",
    "    cv_results = pd.DataFrame(forest_cv.cv_results_)\n",
    "    cv_results.to_csv('cache_forest/forest_gridsearch_results.csv')\n",
    "    \n",
    "# Then display our results in a Pandas dataframe, sorted by\n",
    "# rank based on mean f1 score across 5-fold CV testing:\n",
    "cv_results.sort_values('rank_test_f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We extract our best model and best parameters from our GridSearchCV results.\n",
    "best_forest = forest_cv.best_estimator_\n",
    "best_params = forest_cv.best_params_\n",
    "\n",
    "print(\"Best params:\\n\")\n",
    "for param, val in best_params.items():\n",
    "    print(param,':',val)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "winning_cv_results = cv_results[cv_results['rank_test_f1'] == 1].iloc[1,:]\n",
    "\n",
    "# display accuracy with 95% confidence interval\n",
    "winning_mean_accuracy = winning_cv_results['mean_test_accuracy']\n",
    "std_accuracy = winning_cv_results['std_test_accuracy']\n",
    "print('With %d-fold cross-validation,\\nAccuracy is: %.3f (95%% CI from %.3f to %.3f).' %\n",
    "          (KFOLDS, winning_mean_accuracy,\n",
    "           float(winning_mean_accuracy - 1.96 * std_accuracy),\n",
    "           float(winning_mean_accuracy + 1.96 * std_accuracy)))\n",
    "\n",
    "# display F1 score with 95% confidence interval\n",
    "winning_mean_f1 = winning_cv_results['mean_test_f1']\n",
    "std_f1 = winning_cv_results['std_test_f1']\n",
    "print('The F1 score is: %.3f (95%% CI from %.3f to %.3f).' %\n",
    "          (winning_mean_f1,\n",
    "           float(winning_mean_f1 - 1.96 * std_f1),\n",
    "           float(winning_mean_f1 + 1.96 * std_f1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing our Top 10 Features\n",
    "These features have the highest feature importance scores as found by our best forest model.\n",
    "\n",
    "Unsurprisingly, they tend to include our most general metrics of performance, like average proficiency and grade 7 ela scores of 4 across all students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_prepped.columns\n",
    "feature_importances = best_forest.feature_importances_\n",
    "\n",
    "features_and_importances = pd.DataFrame(feature_importances,features,['Importances'])\n",
    "\n",
    "# Need column names here after the ohe_data step to analyze the results\n",
    "features_and_importances.sort_values('Importances', ascending=False).iloc[1:11,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring results on the test set\n",
    "\n",
    "Now that we have determined our best preprocessing steps and hyperparameters,\n",
    "we evaluate our results on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train on our full training data on a new forest with our best_params\n",
    "# determined by our GridSearchCV\n",
    "# best_forest.fit(train_prepped, train_labels)\n",
    "\n",
    "# And make predictions on our test data\n",
    "# predictions = best_forest.predict(test_prepped)\n",
    "# f1 = f1_score(test_labels, predictions, average='weighted')\n",
    "# f1 = f1_score(test_labels, predictions)\n",
    "# accuracy = np.sum(predictions == test_labels) / len(test_labels)\n",
    "    \n",
    "# print(\"Weighted Average F1 Score: {0:.4f}\".format(f1))\n",
    "# print(\"Accuracy: {0:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations Based on False Positives\n",
    "\n",
    "We will make our recommendations based on our false positives (i.e. schools that our model\n",
    "thinks should be ranked as 'high_registrations', but for whatever reason, aren't)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recombine train and test data into an aggregate dataset\n",
    "X_orig = pd.concat([train_data, test_data], sort=True)  # including all columns (need for display purposes)\n",
    "X_best = pd.concat([train_prepped, test_prepped], sort=True)  # only columns from what ended up in my best model\n",
    "y = np.concatenate((train_labels,test_labels))\n",
    "\n",
    "X_best_npa = np.array(X_best)\n",
    "y_npa = np.array(y)\n",
    "X_pos = X_best[y==1]\n",
    "X_neg = X_best[y==0]\n",
    "\n",
    "# Run k-fold cross-validation with 5 folds 10 times, which means every school is predicted 10 times.\n",
    "folds = 5\n",
    "repeats = 10\n",
    "rskf = RepeatedStratifiedKFold(n_splits=folds, n_repeats=repeats, random_state=207)\n",
    "\n",
    "# Build dataframes for storing predictions, with columns for each k-fold\n",
    "fold_list = []\n",
    "for f in range(1, (folds * repeats) + 1):\n",
    "    fold_list.append('k{}'.format(f))\n",
    "predictions = pd.DataFrame(index=X_best.index, columns=fold_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the Repeated Stratified K Fold, and and fill out the DataFrames\n",
    "counter = 1\n",
    "print('Please be patient...')\n",
    "for train, test in rskf.split(X_best_npa, y_npa):\n",
    "    # TODO: it might be possible to refactor this into util if caller passes\n",
    "    # in a configured pipeline\n",
    "    best_forest.fit(X_best_npa[train], y_npa[train])\n",
    "    predicted_labels = best_forest.predict(X_best_npa[test])\n",
    "    predictions.iloc[test, counter-1] = predicted_labels\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns for predictions and labels\n",
    "predictions['1s'] = predictions.iloc[:,:50].sum(axis=1)\n",
    "predictions['0s'] = (predictions.iloc[:,:50]==0).sum(axis=1)\n",
    "predictions['true'] = y\n",
    "\n",
    "# Create a table of raw results, the vote for truth\n",
    "X_predicted = pd.concat([X_best, predictions['1s'], predictions['0s'],\n",
    "                         pd.DataFrame(y)], axis=1, join_axes=[X_best.index])\n",
    "X_predicted = X_predicted.sort_values(by=['1s', '0s'], ascending=[False, True])\n",
    "\n",
    "# list all false positives that had at least 5/50 votes for the positive label\n",
    "true_negatives = predictions[predictions['true']==0]\n",
    "false_positives = true_negatives[true_negatives['1s'] > 5].sort_values(by='1s', ascending=False)['1s']\n",
    "\n",
    "# join back to full dataset for all columns (including those previously dropped)\n",
    "fp_result = pd.concat([false_positives, X_orig], axis=1, join='inner')\n",
    "fp_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain only the columns of interest for PASSNYC prioritization\n",
    "fp_features = ['1s',\n",
    "              'dbn',\n",
    "              'school_name',\n",
    "              'economic_need_index',\n",
    "              'grade_7_enrollment',\n",
    "              'num_shsat_test_takers',\n",
    "              'pct_test_takers',\n",
    "              'percent_black__hispanic'\n",
    "              ]\n",
    "df_passnyc = fp_result.loc[:,fp_features]\n",
    "\n",
    "# Determine the number of test takers this school would have needed to meet\n",
    "#   the median percentage of high_registrations\n",
    "median_pct = np.median(X_orig[y==1]['pct_test_takers'])/100\n",
    "target_test_takers = np.multiply(df_passnyc['grade_7_enrollment'], median_pct)\n",
    "\n",
    "# Subtract the number of actual test takers from the hypothetical minimum number\n",
    "delta = target_test_takers - df_passnyc['num_shsat_test_takers']\n",
    "\n",
    "# Multiply the delta by the minority percentage of the school to determine how many minority\n",
    "#   students did not take the test\n",
    "minority_delta = np.multiply(delta, df_passnyc['percent_black__hispanic']/100).astype(int)\n",
    "\n",
    "# Add this number to the dataframe, sort descending, and filter to schools with more than five minority students\n",
    "df_passnyc['minority_delta'] = minority_delta\n",
    "df_passnyc = df_passnyc[df_passnyc['minority_delta'] > 5].sort_values(by='minority_delta', ascending=False)\n",
    "\n",
    "# Create a rank order column\n",
    "df_passnyc.insert(0, 'rank', range(1,df_passnyc.shape[0]+1))\n",
    "\n",
    "# Write to CSV\n",
    "df_passnyc.to_csv('results/results.randomforest.csv')\n",
    "\n",
    "df_passnyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
