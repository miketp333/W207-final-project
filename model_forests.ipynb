{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests Notebook\n",
    "[Return to project overview](final_project_overview.ipynb),\n",
    "\n",
    "### Andrew Larimer, Deepak Nagaraj, Daniel Olmstead, Michael Winton (W207-4-Summer 2018 Final Project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and setting options\n",
    "\n",
    "First we import necessary libraries, including our util functions, and set Pandas and Matplotlib options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import graphviz\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from util import our_train_test_split, read_data, get_dummies, \\\n",
    "    print_cv_results, run_model_get_ordered_predictions, create_passnyc_list\n",
    "import pickle\n",
    "\n",
    "# set default options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading our data using util cleanup and imputing\n",
    "\n",
    "Our util module has shared utility functions for cleaning up our data and imputing means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the cleaned, merged, and mean-imputed data from our utility function\n",
    "train_data, test_data, train_labels, test_labels = read_data(do_imputation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll drop some columns that were used to calculate our dependendt variable, as well as our index column, school name strings, and `school_income_estimate` which had too many missing values to fill via imputing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop a few features for the following reasons:\n",
    "#    Used in generating dependent variable: 'num_shsat_test_takers',\n",
    "#        'offers_per_student', 'pct_test_takers'\n",
    "#    Strings or other non-features: 'dbn', 'school_name'\n",
    "#    Too many empty values: 'school_income_estimate'\n",
    "#    Data preserved in other features: 'zip', 'rigorous_instruction_rating',\n",
    "#       'collaborative_teachers_rating', 'supportive_environment_rating',\n",
    "#       'effective_school_leadership_rating',\n",
    "#       'strong_family_community_ties_rating', 'trust_rating'\n",
    "#    Found not to help model: 'district' (or one-hot encoding)\n",
    "\n",
    "FEATURES_TO_DROP = ['dbn', 'school_name', 'zip', 'num_shsat_test_takers',\n",
    "                    'offers_per_student', 'pct_test_takers', 'school_income_estimate',\n",
    "                    'rigorous_instruction_rating','collaborative_teachers_rating',\n",
    "                    'supportive_environment_rating',\n",
    "                    'effective_school_leadership_rating',\n",
    "                    'strong_family_community_ties_rating', 'trust_rating',\n",
    "                    'district']\n",
    "\n",
    "# We'll go ahead and drop total_columns_to_drop columns.\n",
    "train_prepped = train_data.drop(FEATURES_TO_DROP,axis=1)\n",
    "test_prepped = test_data.drop(FEATURES_TO_DROP,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We confirm our resulting data has no more NAs\n",
    "print(\"Confirm total of remaining NAs is: \",np.sum(np.sum(train_prepped.isna())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing a Random Forest Model on Cross-Validation\n",
    "\n",
    "We now move into training our random forest model. To optimize our hyperparameter of how many trees to include in our forest, we use GridSearchCV and take advantage of its cross validation capability to use cross validation against our training set instead of further reducing our data into smaller train and dev sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define into how many groups we'd like to split our test data\n",
    "# for use in cross-validation to evaluate hyperparameters.\n",
    "KFOLDS = 5\n",
    "\n",
    "# We check for previously saved results and a serialized model saved\n",
    "# to disk before re-running GridSearchCV. To force it to run again,\n",
    "# we can comment out the try: & except: or just delete the last saved\n",
    "# results.\n",
    "\n",
    "try:\n",
    "\n",
    "    cv_results = pd.read_csv('cache_forest/forest_gridsearch_results.csv')\n",
    "    with open('cache_forest/pickled_forest','rb') as f:\n",
    "        forest_cv = pickle.load(f)\n",
    "\n",
    "except:\n",
    "\n",
    "    # If no saved results are found, we define our base Random Forest\n",
    "    # Classifier with fixed parameters we don't anticipate adjusting.\n",
    "    # We want to run as many jobs as we have cores at once (which is\n",
    "    # what the -1 input to n_jobs does, and we define our random state\n",
    "    # for reproducibility.)\n",
    "    forest = RandomForestClassifier(n_jobs=-1, class_weight='balanced',\n",
    "                                    random_state=207)\n",
    "\n",
    "    # We define a range of paramters we'd like to try for our forest.\n",
    "    params_to_try = {'n_estimators':[10,30,100,300],\n",
    "                     'max_depth':[None,2,5,7],\n",
    "                     'min_samples_leaf':[1,2,4],\n",
    "                     'min_samples_split':[2,3,5],\n",
    "                     'max_features':[.2,.5,.8]}\n",
    "\n",
    "    # Now we run GridSearchCV on our forest estimator, trying our varying\n",
    "    # numbers of trees and utilizing our cross validation to determine the\n",
    "    # best number of trees across the best number of train/dev cross\n",
    "    # validation splits, using a weighted F1 score as our metric of success.\n",
    "    forest_cv = GridSearchCV(forest, params_to_try, scoring=['f1',\n",
    "                            'accuracy'], refit='f1', cv=KFOLDS,\n",
    "                             return_train_score=False)\n",
    "\n",
    "    # We'll time it and report how long it took to run:\n",
    "    start_time = time.time()\n",
    "    forest_cv.fit(train_prepped, train_labels)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    took = int(end_time - start_time)\n",
    "    print(\"Grid search took {0:d} minutes, {1:d} seconds.\".format(\n",
    "              took // 60, took % 60))\n",
    "\n",
    "    # And pickle our trained model, and save our scores to csv.\n",
    "    with open('cache_forest/pickled_forest','wb') as f:\n",
    "        pickle.dump(forest_cv, f)\n",
    "\n",
    "    cv_results = pd.DataFrame(forest_cv.cv_results_)\n",
    "    cv_results.to_csv('cache_forest/forest_gridsearch_results.csv')\n",
    "    \n",
    "# Then display our results in a Pandas dataframe, sorted by\n",
    "# rank based on mean f1 score across 5-fold CV testing:\n",
    "cv_results.sort_values('rank_test_f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We extract our best model and best parameters from our GridSearchCV results.\n",
    "best_forest = forest_cv.best_estimator_\n",
    "best_params = forest_cv.best_params_\n",
    "\n",
    "# We reiterate our preferred number of cross validation folds if we haven't\n",
    "# had to re-train our model\n",
    "KFOLDS = 5\n",
    "\n",
    "print(\"Best params:\\n\")\n",
    "for param, val in best_params.items():\n",
    "    print(param,':',val)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "winning_cv_results = cv_results[cv_results['rank_test_f1'] == 1].iloc[1,:]\n",
    "\n",
    "# display accuracy with 95% confidence interval\n",
    "winning_mean_accuracy = winning_cv_results['mean_test_accuracy']\n",
    "std_accuracy = winning_cv_results['std_test_accuracy']\n",
    "print('With %d-fold cross-validation,\\nAccuracy is: %.3f (95%% CI from %.3f to %.3f).' %\n",
    "          (KFOLDS, winning_mean_accuracy,\n",
    "           float(winning_mean_accuracy - 1.96 * std_accuracy),\n",
    "           float(winning_mean_accuracy + 1.96 * std_accuracy)))\n",
    "\n",
    "# display F1 score with 95% confidence interval\n",
    "winning_mean_f1 = winning_cv_results['mean_test_f1']\n",
    "std_f1 = winning_cv_results['std_test_f1']\n",
    "print('The F1 score is: %.3f (95%% CI from %.3f to %.3f).' %\n",
    "          (winning_mean_f1,\n",
    "           float(winning_mean_f1 - 1.96 * std_f1),\n",
    "           float(winning_mean_f1 + 1.96 * std_f1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing our Top 10 Features\n",
    "These features have the highest feature importance scores as found by our best forest model.\n",
    "\n",
    "Unsurprisingly, they tend to include our most general metrics of performance, like average proficiency and grade 7 ela scores of 4 across all students.\n",
    "\n",
    "It is interesting to see how heavily the absence and attendance rates factor in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_prepped.columns\n",
    "feature_importances = best_forest.feature_importances_\n",
    "\n",
    "features_and_importances = pd.DataFrame(feature_importances,features,['Importances'])\n",
    "\n",
    "# Need column names here after the ohe_data step to analyze the results\n",
    "features_and_importances.sort_values('Importances', ascending=False).iloc[1:11,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing a few trees from our forest\n",
    "\n",
    "To see what decisions some of our trees are coming to, let's take a look at three random trees out of our group of estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees_in_forest = best_forest.estimators_\n",
    "max_index = len(trees_in_forest)\n",
    "\n",
    "random_indeces = np.random.randint(0, max_index, 3)\n",
    "example_graphs = []\n",
    "\n",
    "for index in random_indeces:\n",
    "    tree_viz = export_graphviz(trees_in_forest[index], proportion=True, filled=True,\n",
    "                               feature_names=train_prepped.columns, rounded=True,\n",
    "                               class_names=['not_high_registrations','high_registrations'],\n",
    "                               out_file=None)\n",
    "    graphviz.Source(source=tree_viz, filename='cache_forest/tree_viz_{0}'.format(index), format='svg').render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the displayed graphs below, the more orange a cell is, the more the samples that pass through it tend to be not in our \"high_registrations\" category. The more blue a cell is, the more it tends to include \"high_registrations.\" We are using the gini measurement of impurity to structure our trees.\n",
    "\n",
    "The samples percentage tells us what percentage of our total samples pass through this node.\n",
    "\n",
    "The value list tells us how many of the samples that have reached this node are in each class. So the first value (value[0]) indicates what proportion of the samples in the node are not high_registrations, and the second value (value[1]) tells us how many are high_registrations. You can see that these values correspond to the coloring of the graph.\n",
    "\n",
    "Then from each node, if a sample meets the condition that titles the node, it travels to the lower left branch. If it does not meed the condition of the node, it travels down the right branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph of Tree #13\n",
    "\n",
    "![Random Graph 0](cache_forest/tree_viz_13.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph of Tree #39\n",
    "\n",
    "![Random Graph 1](cache_forest/tree_viz_39.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph of Tree #77\n",
    "\n",
    "![Random Graph 0](cache_forest/tree_viz_77.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember these are just three out of our total 100 trees that make our ensemble predictor, and each of these trees only have half of the total features in our set. Their variation is what helps 'smooth out the edges' of some of the predictions, to gain the benefits of an ensemble within a single model.\n",
    "\n",
    "All in all, the graph results are to be expected given the features that we found to be important, but the PASSNYC team specifically asked for models that could be explained, and we feel these trees would of course help explain the model's decision-making process clearly to all stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring results on the test set\n",
    "\n",
    "Now that we have determined our best preprocessing steps and hyperparameters,\n",
    "we evaluate our results on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train on our full training data on a new forest with our best_params\n",
    "# determined by our GridSearchCV\n",
    "best_forest.fit(train_prepped, train_labels)\n",
    "predictions = best_forest.predict(test_prepped)\n",
    "\n",
    "# And make predictions on our test data\n",
    "# predictions = best_forest.predict(test_prepped)\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "accuracy = np.sum(predictions == test_labels) / len(test_labels)\n",
    "    \n",
    "print(\"Average F1 Score: {0:.4f}\".format(f1))\n",
    "print(\"Accuracy: {0:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations Based on False Positives\n",
    "\n",
    "We will make our recommendations based on our false positives (i.e. schools that our model\n",
    "thinks should be ranked as 'high_registrations', but for whatever reason, aren't)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_df = run_model_get_ordered_predictions(best_forest, train_data, test_data,\n",
    "                                      train_prepped, test_prepped,\n",
    "                                      train_labels, test_labels)\n",
    "\n",
    "\n",
    "# We now use another util function to generate the list we'll feed to our final\n",
    "# ensemble evaluation.\n",
    "\n",
    "df_passnyc = create_passnyc_list(fp_df, train_data, test_data, train_labels, test_labels)\n",
    "# Write to CSV\n",
    "df_passnyc.to_csv('results/results.randomforest.csv')\n",
    "\n",
    "df_passnyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
