{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Notebook\n",
    "[Return to project overview](final_project_overview.ipynb)\n",
    "\n",
    "\n",
    "### Andrew Larimer, Deepak Nagaraj, Daniel Olmstead, Michael Winton (W207-4-Summer 2018 Final Project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import util\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# set default options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and split class labels into separate array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from CSV\n",
    "df = pd.read_csv('data_merged/combined_data_2018-07-18.csv')\n",
    "\n",
    "# confirm dataset shape looks right\n",
    "print(df.shape)\n",
    "\n",
    "# this will show how many non-null values in each column\n",
    "df.info()\n",
    "\n",
    "# preview a few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **KEY OBSERVATION**: the feature `school_income_estimate` only has non-null values for 132 of 464 records.  We should drop it from further analysis, as imputing its value for the non-null records isn't appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create y variable with labels\n",
    "y = df['high_registrations']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and fit a \"naive\" model\n",
    "For the first model, we'll use all features except SHSAT-related features because they are too correlated with the way we calculated the label.  We'll also drop `school_income_estimate` because it's missing for ~2/3 of the schools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['dbn',\n",
    "             'num_shsat_test_takers',\n",
    "             'offers_per_student',\n",
    "             'pct_test_takers',\n",
    "             'high_registrations',\n",
    "             'school_name',\n",
    "             'school_income_estimate',\n",
    "#              'district',\n",
    "#              'zip',\n",
    "            ]\n",
    "\n",
    "# drop SHSAT-related columns\n",
    "X = df.drop(drop_cols, axis=1)\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values\n",
    "\n",
    "The sklearn estimators assume that all values in an array are numerical, and have meaning, so we need to replace `NaN` values.  We choose to use the column means for this imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values by setting them to the column mean\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X)\n",
    "X_imputed = imp.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding of categorical explanatory variables\n",
    "Columns such as zip code and school district ID, which are integeres should not be fed into an ML model as integers.  Instead, we would need to treat them as factors and perform one-hot encoding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode these features as factors\n",
    "factor_cols = ['district', 'zip']\n",
    "\n",
    "# get indices for these columns\n",
    "factor_col_ids = []\n",
    "for f in factor_cols:\n",
    "    idx = df.columns.get_loc(f)\n",
    "    factor_col_ids.append(idx)\n",
    "factor_col_ids = np.array(factor_col_ids)\n",
    "\n",
    "print(X_imputed.shape)\n",
    "ohe_enc = OneHotEncoder(categorical_features=factor_col_ids, handle_unknown='ignore')\n",
    "X_ohe = ohe_enc.fit_transform(X_imputed)\n",
    "print(X_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and test sets\n",
    "\n",
    "Split into train (80%) and test (20%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and test sets; make sure to stratify\n",
    "X_train, X_test, y_train, y_test = util.our_train_test_split(X_ohe, y, stratify=y)\n",
    "\n",
    "# confirm stratification\n",
    "print('Frac positive class in training set = %.3f' % (np.sum(y_train==1) / len(y_train)))\n",
    "print('Frac positive class in test set = %.3f' % (np.sum(y_test==1) / len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **KEY OBSERVATION**: a hypothetical model that is hard-coded to predict a `negative` result every time would be ~77% accurate.  So, we should not accept any machine-learned model with a lower accuracy than that.  This also suggests that F1 score is a better metric to assess our work since it incorporates both precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a \"naive\" multilayer perceptron model\n",
    "This first \"naive\" model uses all except for the SHSAT-related features, as described above.  We create a pipeline that will be used for k-fold cross-validation.  First, we scale the features, then estimate a multilayer perceptron neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline to run these in sequence\n",
    "n_features = X_train.shape[1]\n",
    "pipe_clf = make_pipeline(StandardScaler(with_mean=False), \n",
    "                   MLPClassifier(hidden_layer_sizes=(n_features,n_features,n_features), max_iter=500))\n",
    "\n",
    "# Do k-fold cross-validation, collecting both \"test\" accuracy and F1 \n",
    "k_folds=10\n",
    "cv_scores = cross_validate(pipe_clf, X_train, y_train, cv=k_folds, scoring=['accuracy','f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display accuracy with 95% confidence interval\n",
    "cv_accuracy = cv_scores['test_accuracy']\n",
    "print ('With %d-fold cross-validation, accuracy is: %.3f (95%% CI from %.3f to %.3f).' %\n",
    "       (k_folds, cv_accuracy.mean(), cv_accuracy.mean() - 1.96 * cv_accuracy.std(),\n",
    "        cv_accuracy.mean() + 1.96 * cv_accuracy.std()))\n",
    "\n",
    "# display F1 score with 95% confidence interval\n",
    "cv_f1 = cv_scores['test_f1']\n",
    "print ('The F1 score is: %.3f (95%% CI from %.3f to %.3f).' %\n",
    "       (cv_f1.mean(), cv_f1.mean() - 1.96 * cv_f1.std(),\n",
    "        cv_f1.mean() + 1.96 * cv_f1.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a \"naive\" model without zip code or school district\n",
    "Next, we will remove the zip and district features and compare accuracy to the model that included one hot-encoded versions of these factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['dbn',\n",
    "             'num_shsat_test_takers',\n",
    "             'offers_per_student',\n",
    "             'pct_test_takers',\n",
    "             'high_registrations',\n",
    "             'school_name',\n",
    "             'school_income_estimate',\n",
    "             'district',\n",
    "             'zip',\n",
    "            ]\n",
    "\n",
    "# drop SHSAT-related columns\n",
    "X = df.drop(drop_cols, axis=1)\n",
    "\n",
    "# impute missing values by setting them to the column mean\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X)\n",
    "X_imputed = imp.transform(X)\n",
    "print(X_imputed.shape)\n",
    "\n",
    "# split into training and test sets; make sure to stratify\n",
    "X_train, X_test, y_train, y_test = util.our_train_test_split(X_imputed, y, stratify=y)\n",
    "\n",
    "# confirm stratification\n",
    "print('Frac positive class in training set = %.3f' % (np.sum(y_train==1) / len(y_train)))\n",
    "print('Frac positive class in test set = %.3f' % (np.sum(y_test==1) / len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline to run these in sequence\n",
    "n_features = X_train.shape[1]\n",
    "pipe_clf = make_pipeline(StandardScaler(with_mean=False), \n",
    "                   MLPClassifier(hidden_layer_sizes=(n_features,n_features,n_features), max_iter=500))\n",
    "\n",
    "# Do k-fold cross-validation, collecting both \"test\" accuracy and F1 \n",
    "k_folds=10\n",
    "cv_scores = cross_validate(pipe_clf, X_train, y_train, cv=k_folds, scoring=['accuracy','f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display accuracy with 95% confidence interval\n",
    "cv_accuracy = cv_scores['test_accuracy']\n",
    "print ('With %d-fold cross-validation, accuracy is: %.3f (95%% CI from %.3f to %.3f).' %\n",
    "       (k_folds, cv_accuracy.mean(), cv_accuracy.mean() - 1.96 * cv_accuracy.std(),\n",
    "        cv_accuracy.mean() + 1.96 * cv_accuracy.std()))\n",
    "\n",
    "# display F1 score with 95% confidence interval\n",
    "cv_f1 = cv_scores['test_f1']\n",
    "print ('The F1 score is: %.3f (95%% CI from %.3f to %.3f).' %\n",
    "       (cv_f1.mean(), cv_f1.mean() - 1.96 * cv_f1.std(),\n",
    "        cv_f1.mean() + 1.96 * cv_f1.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **KEY OBSERVATION**: while the accuracy is similar when we exclude zip code and school district, the F1 score is substantially less, with a 95% confidence interval that nearly spans the interval 0-1.  This suggests that it's important to keep these factors in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a \"race-blind\" multilayer perceptron model\n",
    "Because we know there's an existing bias problem in the NYC schools, in that the demographics of the test taking population have been getting more homogenous, and the explicit goal of PASSNYC is to make the pool more diverse, we want to train a model that excludes most demographic features.  This would enable us to train a \"race-blind\" model.  \n",
    "\n",
    "### Preprocess new X_train and X_test datasets\n",
    "We will remove all explicitly demographic columns, as well as economic factors and zip code, which are likely highly correlated with demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop SHSAT-related columns\n",
    "drop_cols = ['dbn',\n",
    "             'num_shsat_test_takers',\n",
    "             'offers_per_student',\n",
    "             'pct_test_takers',\n",
    "             'high_registrations',\n",
    "             'school_name',\n",
    "             'school_income_estimate'\n",
    "            ]\n",
    "X = df.drop(drop_cols, axis=1)\n",
    "\n",
    "# drop additional (demographic) columns\n",
    "race_cols = ['percent_ell',\n",
    "             'percent_asian',\n",
    "             'percent_black',\n",
    "             'percent_hispanic',\n",
    "             'percent_black__hispanic',\n",
    "             'percent_white',\n",
    "             'economic_need_index',\n",
    "             'zip'\n",
    "             ]\n",
    "X_race_blind = X.drop(race_cols, axis=1)\n",
    "\n",
    "# impute missing values by setting them to the column mean\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X_race_blind)\n",
    "X_race_blind_imputed = imp.transform(X_race_blind)\n",
    "\n",
    "# one-hot encode these features as factors\n",
    "factor_cols = ['district']\n",
    "\n",
    "# get indices for these columns\n",
    "factor_col_ids = []\n",
    "for f in factor_cols:\n",
    "    idx = X_race_blind.columns.get_loc(f)\n",
    "    factor_col_ids.append(idx)\n",
    "factor_col_ids = np.array(factor_col_ids)\n",
    "\n",
    "# perform one hot encoding\n",
    "ohe_enc = OneHotEncoder(categorical_features=factor_col_ids, handle_unknown='ignore')\n",
    "X_race_blind_ohe = ohe_enc.fit_transform(X_race_blind_imputed)\n",
    "\n",
    "# split into training and test sets; make sure to stratify\n",
    "X_train, X_test, y_train, y_test = util.our_train_test_split(X_race_blind_ohe, y, stratify=y)\n",
    "\n",
    "# confirm stratification\n",
    "print('Frac positive class in training set = %.3f' % (np.sum(y_train==1) / len(y_train)))\n",
    "print('Frac positive class in test set = %.3f' % (np.sum(y_test==1) / len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline to run these in sequence\n",
    "n_features = X_train.shape[1]\n",
    "pipe_clf = make_pipeline(StandardScaler(with_mean=False), \n",
    "                   MLPClassifier(hidden_layer_sizes=(n_features,n_features,n_features), max_iter=500))\n",
    "\n",
    "# Do k-fold cross-validation, collecting both \"test\" accuracy and F1 \n",
    "k_folds=10\n",
    "cv_scores = cross_validate(pipe_clf, X_train, y_train, cv=k_folds, scoring=['accuracy','f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display accuracy with 95% confidence interval\n",
    "cv_accuracy = cv_scores['test_accuracy']\n",
    "print ('With %d-fold cross-validation, accuracy is: %.3f (95%% CI from %.3f to %.3f).' %\n",
    "       (k_folds, cv_accuracy.mean(), cv_accuracy.mean() - 1.96 * cv_accuracy.std(),\n",
    "        cv_accuracy.mean() + 1.96 * cv_accuracy.std()))\n",
    "\n",
    "# display F1 score with 95% confidence interval\n",
    "cv_f1 = cv_scores['test_f1']\n",
    "print ('The F1 score is: %.3f (95%% CI from %.3f to %.3f).' %\n",
    "       (cv_f1.mean(), cv_f1.mean() - 1.96 * cv_f1.std(),\n",
    "        cv_f1.mean() + 1.96 * cv_f1.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **KEY OBSERVATION**: the F1 score for the race-blind model also have a 95% confidence interval that nearly spans the whole range from 0-1.  Of the models we have tested, the original \"naive\" model (with the most features) performs better than our race-blind model, or our model that excluded only zip and district."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with hidden layer parameters in the \"naive\" model\n",
    "Next, we will return to the feature set of the original \"naive\" model, but will explore different numbers of h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final test set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predict = mlp.predict(X_test)\n",
    "\n",
    "# print(confusion_matrix(y_test,y_predict))\n",
    "# print(classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
