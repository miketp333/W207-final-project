{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Notebook\n",
    "[Return to project overview](final_project_overview.ipynb)\n",
    "\n",
    "\n",
    "### Andrew Larimer, Deepak Nagaraj, Daniel Olmstead, Michael Winton (W207-4-Summer 2018 Final Project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import util\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, RepeatedStratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# set default options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and split class labels into separate array\n",
    "\n",
    "Our utility function reads the merged dataset, imputes the column mean for missing numeric values, and then performs a stratified train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = util.read_data(do_imputation=True)\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **KEY OBSERVATION**: a hypothetical model that is hard-coded to predict a `negative` result every time would be ~77% accurate.  So, we should not accept any machine-learned model with a lower accuracy than that.  This also suggests that F1 score is a better metric to assess our work since it incorporates both precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **KEY OBSERVATION**: the feature `school_income_estimate` only has non-null values for 104 of 371 records in the training data.  We should drop it from further analysis, as imputing its value for the non-null records isn't appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function to estimate MLP models and report results\n",
    "We create a pipeline that will be used for k-fold cross-validation.  First, we scale the features, then perform dimensionality reduction by PCA if requested by the caller, then estimate a multilayer perceptron neural network with the hidden layers defined by the caller. Once the pipeline is built, we perform cross-validation, print the results, and return key statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_mlp(train_data, train_labels, n_pca=None,\n",
    "                 hidden_layers=None, k_folds=5, max_iter=1000, print_results=True):\n",
    "\n",
    "    # if tuple describing hidden layer nodes isn't provided, set default\n",
    "    if not hidden_layers:\n",
    "        n_features = train_data.shape[1]\n",
    "        hidden_layers = (n_features,n_features,n_features)\n",
    "        \n",
    "    # build pipelines, with or without PCA as appropriate\n",
    "    if n_pca:\n",
    "        # create a pipeline to run StandardScaler and MLP\n",
    "        print('Estimating pipeline with PCA; hidden layers:',hidden_layers)\n",
    "        pipeline = make_pipeline(StandardScaler(with_mean=False), \n",
    "                                 PCA(n_components=n_pca, random_state=207),\n",
    "                                 MLPClassifier(hidden_layer_sizes=hidden_layers,\n",
    "                                               max_iter=max_iter, random_state=207))\n",
    "    else:\n",
    "        # create a pipeline to run StandardScaler and MLP\n",
    "        print('Estimating pipeline without PCA; hidden layers:',hidden_layers)\n",
    "        pipeline = make_pipeline(StandardScaler(with_mean=False), \n",
    "                                 MLPClassifier(hidden_layer_sizes=hidden_layers,\n",
    "                                               max_iter=max_iter, random_state=207))\n",
    "\n",
    "    # Do k-fold cross-validation, collecting both \"test\" accuracy and F1 \n",
    "    cv_scores = cross_validate(pipeline, train_data, train_labels, cv=k_folds, scoring=['accuracy','f1'])\n",
    "    if print_results:\n",
    "        util.print_cv_results(cv_scores)\n",
    "        \n",
    "    # extract and return accuracy, F1\n",
    "    cv_accuracy = cv_scores['test_accuracy']\n",
    "    cv_f1 = cv_scores['test_f1']\n",
    "    return (cv_accuracy.mean(), cv_accuracy.std(), cv_f1.mean(), cv_f1.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and fit a \"naive\" model\n",
    "For the first model, we'll use all features except SHSAT-related features because they are too correlated with the way we calculated the label.  We'll also drop `school_income_estimate` because it's missing for ~2/3 of the schools.  We drop zip code (too granular to have many schools per zip) in favor of the indicator variables `in_[borough]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['dbn',\n",
    "             'num_shsat_test_takers',\n",
    "             'offers_per_student',\n",
    "             'pct_test_takers',\n",
    "             'school_name',\n",
    "             'school_income_estimate',\n",
    "             'zip'\n",
    "            ]\n",
    "\n",
    "# drop SHSAT-related columns\n",
    "train_data_naive = train_data.drop(drop_cols, axis=1)\n",
    "test_data_naive = test_data.drop(drop_cols, axis=1)\n",
    "\n",
    "print(train_data_naive.shape)\n",
    "train_data_naive.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode the categorical explanatory variables\n",
    "Columns such as zip code and school district ID, which are integers, should not be fed into an ML model as integers.  Instead, we need to treat them as factors and perform one-hot encoding.  Since we have already removed zip code from our dataframe (in favor of boroughs), we only need to one hot encode `district`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_naive_ohe, test_data_naive_ohe = util.get_dummies(train_data_naive, test_data_naive,\n",
    "                                                             factor_cols=['district'])\n",
    "train_data_naive_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the \"naive\" multilayer perceptron model\n",
    "This first \"naive\" model uses all except for the SHSAT-related features, as described above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard return vals; only print results\n",
    "(_,_,_,_) = estimate_mlp(train_data_naive_ohe, train_labels, k_folds=5, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a \"naive\" model without location (zip, borough, or district)\n",
    "Next, we remove all location features (zip, borough, and district) to compare accuracy to the prior model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['dbn',\n",
    "             'num_shsat_test_takers',\n",
    "             'offers_per_student',\n",
    "             'pct_test_takers',\n",
    "             'school_name',\n",
    "             'school_income_estimate',\n",
    "             'district',\n",
    "             'zip',\n",
    "             'in_bronx',\n",
    "             'in_brooklyn',\n",
    "             'in_manhattan',\n",
    "             'in_queens',\n",
    "             'in_staten'\n",
    "            ]\n",
    "\n",
    "# drop SHSAT-related columns + district, zip, borough\n",
    "train_data_naive_nozip = train_data.drop(drop_cols, axis=1)\n",
    "test_data_naive_nozip = test_data.drop(drop_cols, axis=1)\n",
    "\n",
    "print(train_data_naive_nozip.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the \"naive\" multilayer perceptron model without location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard return vals; only print results\n",
    "(_,_,_,_) = estimate_mlp(train_data_naive_nozip, train_labels, k_folds=5, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the accuracy is similar to the first model, the F1 score is lower.  This suggests that it's important to keep location-oriented factors in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a \"race-blind\" multilayer perceptron model\n",
    "Because we know there's an existing bias problem in the NYC schools, in that the demographics of the test taking population have been getting more homogenous, and the explicit goal of PASSNYC is to make the pool more diverse, we want to train a model that excludes most demographic features.  This would enable us to train a \"race-blind\" model.  \n",
    "\n",
    "### Preprocess new X_train and X_test datasets\n",
    "We will remove all explicitly demographic columns, as well as economic factors, borough, and zip code, which are likely highly correlated with demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop SHSAT-related columns\n",
    "drop_cols = ['dbn',\n",
    "             'num_shsat_test_takers',\n",
    "             'offers_per_student',\n",
    "             'pct_test_takers',\n",
    "             'school_name',\n",
    "             'school_income_estimate'\n",
    "            ]\n",
    "train_data_race_blind = train_data.drop(drop_cols, axis=1)\n",
    "test_data_race_blind = test_data.drop(drop_cols, axis=1)\n",
    "\n",
    "# drop additional (demographic) columns\n",
    "race_cols = ['percent_ell',\n",
    "             'percent_asian',\n",
    "             'percent_black',\n",
    "             'percent_hispanic',\n",
    "             'percent_black__hispanic',\n",
    "             'percent_white',\n",
    "             'economic_need_index',\n",
    "             'zip',\n",
    "             'in_bronx',\n",
    "             'in_brooklyn',\n",
    "             'in_manhattan',\n",
    "             'in_queens',\n",
    "             'in_staten'\n",
    "             ]\n",
    "train_data_race_blind = train_data_race_blind.drop(race_cols, axis=1)\n",
    "test_data_race_blind = test_data_race_blind.drop(race_cols, axis=1)\n",
    "\n",
    "# one-hot encode these features as factors\n",
    "factor_cols = ['district']\n",
    "train_data_race_blind_ohe, test_data_race_blind_ohe =util.get_dummies(train_data_race_blind,\n",
    "                                                                      test_data_race_blind, factor_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the \"race blind\" multilayer perceptron model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard return vals; only print results\n",
    "(_,_,_,_) = estimate_mlp(train_data_race_blind_ohe, train_labels, k_folds=5, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score for the race-blind model declines further when we remove these features.  Of the models we have tested, the original \"naive\" model (with the most features) performs better than our race-blind model, or our model that excluded only zip and district."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with dimensionality reduction via PCA\n",
    "Since manual feature selection performed poorly, it doesn't seem to be a promising approach.  Instead, we will experiment with Principal Component Analysis for dimensionality reduction, starting with the \"naive\" set of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of principal components to achieve 90% explained variance\n",
    "n_pca = util.get_num_pcas(train_data_naive, var_explained=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using %d principal components' % (n_pca)) # currently n_pca=69\n",
    "\n",
    "# discard return vals; only print results\n",
    "(_,_,_,_) = estimate_mlp(train_data_naive_ohe, train_labels, n_pca=n_pca, k_folds=5, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying PCA for dimensionality reduction, in contrast to manual feature selection, seemed to improve our results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use grid search to identify best set of hidden layer parameters\n",
    "Since the usage of PCA seemed to improve our F1 score (and tighten its confidence interval), we will proceed to optimize the hidden layer parameters while using PCA and the original \"naive\" feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running grid search for different combinations of neural network parameters is slow.\n",
    "# If results already exist as a file, load them instead of re-running.\n",
    "try:\n",
    "    grid_search_results = pd.read_csv('cache_neuralnet/gridsearch_results.csv')\n",
    "    print('Loaded grid search results from file.')\n",
    "except FileNotFoundError:\n",
    "    print('Performing grid search for best hidden layer parameters.')\n",
    "\n",
    "    # We'll time it and report how long it took to run:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # numbers of hidden nodes = these multipliers * # features\n",
    "    n_features = train_data_naive_ohe.shape[1]\n",
    "#     fraction = [0.25, 0.5]\n",
    "    fraction = [0.25, 0.5, 1.0, 1.5, 2.0]\n",
    "    n_layer_features = (int(f * n_features) for f in fraction)\n",
    "    n_nodes = list(n_layer_features)\n",
    "\n",
    "    # create list of tuples of hidden layer param permutations\n",
    "    # only explore up to 4 hidden layers\n",
    "    hl_param_candidates = []\n",
    "    for h1 in n_nodes:\n",
    "        hl_param_candidates.append((h1))\n",
    "        for h2 in n_nodes:\n",
    "            hl_param_candidates.append((h1,h2))\n",
    "            for h3 in n_nodes:\n",
    "                hl_param_candidates.append((h1,h2,h3))\n",
    "                for h4 in n_nodes:\n",
    "                    hl_param_candidates.append((h1,h2,h3,h4))\n",
    "    \n",
    "    # train an MLP model and perform cross-validation for each parameter set\n",
    "    print('Estimating %d MLP models. This will take time!\\n' % (len(hl_param_candidates)))\n",
    "    tmp_results = []        \n",
    "    for hl in hl_param_candidates:\n",
    "        tmp_acc, tmp_acc_std, tmp_f1, tmp_f1_std = estimate_mlp(train_data_naive_ohe, train_labels, \n",
    "                                                                hidden_layers=hl, n_pca=n_pca,\n",
    "                                                                k_folds=5, max_iter=1000, print_results=False)\n",
    "        tmp_results.append((hl, tmp_acc, tmp_acc - 1.96 * tmp_acc_std, tmp_acc + 1.96 * tmp_acc_std,\n",
    "                                    tmp_f1, tmp_f1 - 1.96 * tmp_f1_std, tmp_f1 + 1.96 * tmp_f1_std))\n",
    "\n",
    "    # calculated elapsed time\n",
    "    end_time = time.time()\n",
    "    took = int(end_time - start_time)\n",
    "    print(\"Grid search took {0:d} minutes, {1:d} seconds.\".format(took // 60, took % 60))\n",
    "\n",
    "    # convert results to a dataframe for easier display\n",
    "    grid_search_results = pd.DataFrame(tmp_results)\n",
    "    grid_search_results.columns=(['Hidden Layers','Accuracy','Acc Lower CI', 'Acc Upper CI','F1','F1 Lower CI','F1 Upper CI'])\n",
    "    grid_search_results.to_csv('cache_neuralnet/gridsearch_results.csv', index=False)\n",
    "\n",
    "# Display grid search results\n",
    "grid_search_results.sort_values(by='F1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put best grid search params into a varaiable\n",
    "best_param_idx = grid_search_results['F1'].idxmax()\n",
    "\n",
    "try:  # this is needed when loading from file\n",
    "    best_hl_params = eval(grid_search_results['Hidden Layers'][best_param_idx])\n",
    "except TypeError:  # eval isn't needed when results are still in memory\n",
    "    best_hl_params = grid_search_results['Hidden Layers'][best_param_idx]\n",
    "    \n",
    "best_hl_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate \"out-of-sample\" test set accuracy\n",
    "At this point we can use our \"best\" model parameters to classify our test set, and compare to true labels.\n",
    "\n",
    "> NOTE: This code was left commented out until after hyperparameter optimization was complete.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pipeline with optimal parameters\n",
    "pipeline = make_pipeline(StandardScaler(with_mean=False), \n",
    "                     PCA(n_components=n_pca, random_state=207),\n",
    "                     MLPClassifier(hidden_layer_sizes=best_hl_params,\n",
    "                                   max_iter=1000, random_state=207))\n",
    "pipeline.fit(train_data_naive_ohe, train_labels)\n",
    "test_predict = pipeline.predict(test_data_naive_ohe)\n",
    "\n",
    "print('Test set accuracy: %.2f\\n' % (np.mean(test_predict==test_labels)))\n",
    "print('Confusion matrix:')\n",
    "cm = confusion_matrix(test_labels, test_predict)\n",
    "print(cm)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print('True negatives: %d' % (tn))\n",
    "print('True positives: %d' % (tp))\n",
    "print('False negatives: %d' % (fn))\n",
    "print('False positives: %d\\n' % (fp))\n",
    "print(classification_report(test_labels, test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze predictions to make recommendations to PASSNYC\n",
    "Lastly, we will prioritize the schools based on features that align with the PASSNYC diversity-oriented mission.  We recombine our training and test sets and run k-fold cross-validation with 5 folds 10 times, which means every school is predicted 10 times.  We look at what fraction of these 10 times each school was predicted to be a high registration school and compare this to the true label.\n",
    "\n",
    "We then apply a ranking algorithm:\n",
    "1. Determine the number of test takers a school would have needed to meet the median percentage of high_registrations\n",
    "2. Subtract the number of actual test takers from the hypothetical minimum number.  (Note that for ranking purposes, it is acceptable for this to be a negative number.)\n",
    "3. Multiply the delta by the minority percentage of the school to estimate how many minority students might have taken the test if the school were a median high-registration school.\n",
    "4. Multiply this delta by the fraction of positive votes as a proxy for confidence in the prediction.\n",
    "5. Sort (descending) by this ranking.\n",
    "\n",
    "This analysis attempts to capture a set of schools which have a lot in common with high registration schools, but for some reason fall short.  As a result, we believe these are good candidates for the PASSNYC organization to engage with, as investing in programs with these schools may be more highly to payoff with increase registration rates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a pipeline with the best parameters\n",
    "pipeline = make_pipeline(StandardScaler(with_mean=False), \n",
    "                         PCA(n_components=n_pca, random_state=207),\n",
    "                         MLPClassifier(hidden_layer_sizes=best_hl_params,\n",
    "                                       max_iter=1000, random_state=207))\n",
    "\n",
    "# Let us look at what schools the model classified as positive, but were actually negative.  \n",
    "# These are the schools we should target, because the model thinks they should have high SHSAT registrations,\n",
    "# but in reality they do not.\n",
    "# call our utility function to get predictions for all observations (train and test)\n",
    "print('Be patient...')\n",
    "predictions = util.run_model_get_ordered_predictions(pipeline, train_data, test_data,\n",
    "                                                     train_data_naive_ohe, test_data_naive_ohe,\n",
    "                                                     train_labels, test_labels)\n",
    "\n",
    "# from these results, calculate a ranking of the schools that we can provide to PASSNYC.\n",
    "df_passnyc = util.create_passnyc_list(predictions, train_data, test_data, train_labels, test_labels)\n",
    "\n",
    "# Write to CSV\n",
    "df_passnyc.to_csv('results/results.neuralnet.csv')\n",
    "\n",
    "# Display results\n",
    "df_passnyc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-hoc comparison of prioritization score vs. economic need index\n",
    "Even though economic need index was not an explicit factor in our post-classification prioritization scoring/ranking system, it is interesting to observe that there is some correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_passnyc['score']\n",
    "y = df_passnyc['economic_need_index']\n",
    "sns.regplot(x='score', y='economic_need_index', data=df_passnyc,\n",
    "           fit_reg=True, x_jitter=1, scatter_kws={'alpha': 0.5, 's':4})\n",
    "# plt.scatter(x, y)\n",
    "# plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "plt.xlabel('PASSNYC Priorization \"Score\"')\n",
    "plt.ylabel('Economic Need Index')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
