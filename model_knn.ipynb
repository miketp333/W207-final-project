{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors\n",
    "\n",
    "### Andrew Larimer, Deepak Nagaraj, Daniel Olmstead, Michael Winton\n",
    "\n",
    "#### W207-4-Summer 2018 Final Project\n",
    "\n",
    "[Return to project overview](final_project_overview.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we attempt to classify the PASSNYC data via K-Nearest Neighbors algorithm.\n",
    "\n",
    "### Reading data\n",
    "Let us do some initial imports and set up the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import util\n",
    "\n",
    "# set default options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train-test split\n",
    "train_data, test_data, train_labels, test_labels = util.read_data()\n",
    "\n",
    "print(\"Train data shape: %s\" % str(train_data.shape))\n",
    "print(\"Test data shape: %s\" % str(test_data.shape))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "We will now select some features from the above dataset.\n",
    "\n",
    "We ignore the following demographic indicators:\n",
    "* school_name\n",
    "* zip\n",
    "* district\n",
    "* community_school\n",
    "* economic_need_index\n",
    "* school_income_estimate\n",
    "* percent_ell\n",
    "* percent_black\n",
    "* percent_hispanic\n",
    "* percent_asian\n",
    "* percent_white\n",
    "* percent_of_students_chronically_absent\n",
    "\n",
    "We also ignore the following columns because they proxy output variable:\n",
    "* num_shsat_test_takers\n",
    "* offers_per_student\n",
    "* pct_test_takers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate this list again:\n",
    "# Take above (markdown) list and store it in say ~/tmp/col_list.  Then:\n",
    "# cat  ~/tmp/col_list | cut -d\" \" -f2 | sed -E 's/^(.*)$/\"\\1\"/' | tr '\\n' ', '\n",
    "\n",
    "drop_cols = [\n",
    "    # non-numeric\n",
    "    'dbn',\n",
    "    # correlated with outcome variable\n",
    "    'num_shsat_test_takers',\n",
    "    'offers_per_student',\n",
    "    'pct_test_takers',\n",
    "    # demographic or correlated with demographics\n",
    "    'school_name',\n",
    "    'zip',\n",
    "    'district',\n",
    "    'community_school',\n",
    "    'economic_need_index',\n",
    "    'school_income_estimate',\n",
    "    'percent_ell',\n",
    "    'percent_black',\n",
    "    'percent_black__hispanic',\n",
    "    'percent_hispanic',\n",
    "    'percent_asian',\n",
    "    'percent_white',\n",
    "    'grade_7_ela_4s_american_indian_or_alaska_native',\n",
    "    'grade_7_ela_4s_black_or_african_american',\n",
    "    'grade_7_ela_4s_hispanic_or_latino',\n",
    "    'grade_7_ela_4s_asian_or_pacific_islander',\n",
    "    'grade_7_ela_4s_white',\n",
    "    'grade_7_ela_4s_multiracial',\n",
    "    'grade_7_ela_4s_limited_english_proficient',\n",
    "    'grade_7_ela_4s_economically_disadvantaged',\n",
    "    'grade_7_math_4s_american_indian_or_alaska_native',\n",
    "    'grade_7_math_4s_black_or_african_american',\n",
    "    'grade_7_math_4s_hispanic_or_latino',\n",
    "    'grade_7_math_4s_asian_or_pacific_islander',\n",
    "    'grade_7_math_4s_white',\n",
    "    'grade_7_math_4s_multiracial',\n",
    "    'grade_7_math_4s_limited_english_proficient',\n",
    "    'grade_7_math_4s_economically_disadvantaged',\n",
    "]\n",
    "perf_train_data = train_data.drop(drop_cols, axis=1)\n",
    "perf_train_data.info()\n",
    "perf_train_data_nonull = perf_train_data.fillna(perf_train_data.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors Classification\n",
    "\n",
    "We will now run KNN prediction on the dataset, with the default K value (=5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler().fit(perf_train_data_nonull)\n",
    "rescaledX = scaler.transform(perf_train_data_nonull)\n",
    "y = train_labels.values.ravel()\n",
    "clf = KNeighborsClassifier()\n",
    "\n",
    "# Do k-fold cross-validation, collecting both \"test\" accuracy and F1 \n",
    "k_folds = 10\n",
    "cv_scores = cross_validate(clf, rescaledX, y, cv=k_folds, scoring=['accuracy','f1'])\n",
    "util.print_cv_results(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get accuracy of 83% and F1 score of 0.58.  Let us experiment with various values of $k$ to see which gives the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(MinMaxScaler(), \n",
    "                         KNeighborsClassifier())\n",
    "n_neighbors = list(range(1, 15))\n",
    "estimator = GridSearchCV(pipeline,\n",
    "                        dict(kneighborsclassifier__n_neighbors=n_neighbors),\n",
    "                        cv=10, n_jobs=2, scoring='f1')\n",
    "estimator.fit(perf_train_data_nonull, y)\n",
    "\n",
    "print(\"Best no. of neighbors: %d (with best f1: %.3f)\" % \n",
    "      (estimator.best_params_['kneighborsclassifier__n_neighbors'], \n",
    "       estimator.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best F1 score is 0.62 at $k=3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN with select features\n",
    "\n",
    "We will now attempt to do some feature selection, followed by running KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(MinMaxScaler(), \n",
    "                         SelectFromModel(ExtraTreesClassifier(random_state=207)))\n",
    "pipeline.fit_transform(perf_train_data_nonull, y)\n",
    "selected_features = pipeline.steps[1][1].get_support()\n",
    "perf_train_data_nonull.columns[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_train_data_nonull_sel_cols = ['student_attendance_rate', 'percent_of_students_chronically_absent',\n",
    "       'student_achievement_rating', 'average_ela_proficiency',\n",
    "       'average_math_proficiency', 'grade_7_math_4s_all_students',\n",
    "       'number_of_students_social_studies', 'average_class_size_science']\n",
    "perf_train_data_nonull_sel = perf_train_data_nonull[perf_train_data_nonull_sel_cols]\n",
    "scaler = MinMaxScaler().fit(perf_train_data_nonull_sel)\n",
    "rescaledX = scaler.transform(perf_train_data_nonull_sel)\n",
    "y = train_labels.values.ravel()\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Do k-fold cross-validation, collecting both \"test\" accuracy and F1 \n",
    "k_folds = 10\n",
    "cv_scores = cross_validate(clf, rescaledX, y, cv=k_folds, scoring=['accuracy','f1'])\n",
    "util.print_cv_results(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score falls by about 4%.  We can ignore this set and use the original set instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN with reduced dimensions\n",
    "\n",
    "We will next attempt to reduce dimensions via PCA, followed by KNN.\n",
    "\n",
    "First, we will attempt to find the best number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_explained_variance_ratios = []\n",
    "for n in range(1, 15):\n",
    "    pipeline = make_pipeline(StandardScaler(), \n",
    "                            PCA(n_components=n, random_state=207))\n",
    "    pipeline.fit_transform(perf_train_data_nonull)\n",
    "    pca = pipeline.steps[1][1]\n",
    "    cum_explained_variance_ratios.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.plot(np.array(cum_explained_variance_ratios))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will select first 3 components, which already explain more than 70% of variance.  The slope of the graph goes down after this, indicating that remaining components are not as informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(StandardScaler(), \n",
    "                         PCA(n_components=3, random_state=207),\n",
    "                         KNeighborsClassifier())\n",
    "\n",
    "n_neighbors = list(range(1, 10))\n",
    "estimator = GridSearchCV(pipeline,\n",
    "                        dict(kneighborsclassifier__n_neighbors=n_neighbors),\n",
    "                        cv=10)\n",
    "estimator.fit(perf_train_data_nonull, y)\n",
    "\n",
    "print(\"Best no. of neighbors: %d (with best f1: %.3f)\" % \n",
    "      (estimator.best_params_['kneighborsclassifier__n_neighbors'], \n",
    "       estimator.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that F1 score now goes up quite a bit, to 0.85.  We lose some interpretability, because we're dealing with a lower-dimensional hyperplane, but the model actually is more functional now, as shown by the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
